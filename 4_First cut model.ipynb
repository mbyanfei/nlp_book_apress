{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 17 to Code 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('airline_dataset.csv',low_memory=False,encoding = 'latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When can I web check-in?</td>\n",
       "      <td>check in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>want to check in</td>\n",
       "      <td>check in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>please check me in</td>\n",
       "      <td>check in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check in</td>\n",
       "      <td>check in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my flight is tomm can I check in</td>\n",
       "      <td>check in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               line     class\n",
       "0          When can I web check-in?  check in\n",
       "1                  want to check in  check in\n",
       "2                please check me in  check in\n",
       "3                          check in  check in\n",
       "4  my flight is tomm can I check in  check in"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of \"class\" or intent count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "login        105\n",
       "other         79\n",
       "baggage       76\n",
       "check in      61\n",
       "greetings     45\n",
       "thanks        16\n",
       "cancel        16\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"line\"]=df[\"line\"].str.lower().str.lstrip().str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TFIDF Vectorizer. Get a TFIDF Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF computes term frequency – inverse document frequency. This statistic for a given word indicates how important it is as compared to other words in the corpus. The word that comes in fewer documents will have a higher TF-IDF score and the words that comes in all the documents will have lower TF-IDF score\n",
    "\n",
    "1.\tTF (Term Frequency) in this formula represents the number of times a “word” (token or feature) appears in a document normalized by number of words in the document\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "2.\tIDF (Inverse Document Freqeuncy) represents the number of times the given word occurs in a corpus. \n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "3.\tFinal value is obtained by multiplying TF and IDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “TfidfVectorizer” object has few important parameters to reduce the sparsity of the matrix (number of columns) - \n",
    "1.\tmindf - this is the minimum number of documents a word should be present\n",
    "2.\tStop_words - an option to provide stop words or not.\n",
    "The “Analyzer” can be word or character depending on the type of token we want. We would like to have words for the problem at hand.\n",
    "The options on ngram_range provides any length of ngrams (bigrams, trigrams etc). You could have tokens range from bigrams to trigrams for instance by using ngram_range(2,3). Once we get a vectorizer object we fit it on the corpus and get a sparse matrix. The sparse matrix can then be converted to dense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.0,analyzer=u'word',ngram_range=(1, 1),stop_words=None)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"line\"])\n",
    "tf1= tfidf_matrix.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command tfidf_vectorizer.vocabulary lists the set of tokens done by the vectorizer. As we can see there are a bunch of numbers and dates which may not add much value to the analysis but if they are normalized into entities like dates, months etc can be of a lot of value. We will now see data normalization in the next section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'when': 215,\n",
       " 'can': 43,\n",
       " 'web': 210,\n",
       " 'check': 53,\n",
       " 'in': 103,\n",
       " 'want': 206,\n",
       " 'to': 194,\n",
       " 'please': 161,\n",
       " 'me': 129,\n",
       " 'my': 138,\n",
       " 'flight': 72,\n",
       " 'is': 107,\n",
       " 'tomm': 195,\n",
       " 'not': 146,\n",
       " 'able': 10,\n",
       " 'next': 142,\n",
       " 'week': 211,\n",
       " 'how': 102,\n",
       " 'onine': 153,\n",
       " 'online': 154,\n",
       " 'before': 34,\n",
       " 'checking': 55,\n",
       " 'hours': 101,\n",
       " 'am': 21,\n",
       " 'flying': 75,\n",
       " 'on': 151,\n",
       " '26th': 5,\n",
       " 'of': 149,\n",
       " 'this': 188,\n",
       " 'month': 135,\n",
       " '20th': 3,\n",
       " 'monday': 133,\n",
       " 'now': 147,\n",
       " 'getting': 85,\n",
       " 'late': 117,\n",
       " 'the': 185,\n",
       " 'airport': 16,\n",
       " 'pnr': 162,\n",
       " '65321': 7,\n",
       " 'checkin': 54,\n",
       " 'get': 84,\n",
       " 'internet': 106,\n",
       " 'what': 214,\n",
       " 'free': 79,\n",
       " 'baggage': 29,\n",
       " 'allowance': 20,\n",
       " 'much': 137,\n",
       " 'have': 94,\n",
       " '35': 6,\n",
       " 'kg': 113,\n",
       " 'should': 174,\n",
       " 'pay': 158,\n",
       " 'carry': 49,\n",
       " 'lots': 123,\n",
       " 'bags': 32,\n",
       " 'till': 192,\n",
       " 'many': 127,\n",
       " 'are': 24,\n",
       " 'upto': 201,\n",
       " 'weight': 212,\n",
       " 'number': 148,\n",
       " 'carrying': 50,\n",
       " 'travelling': 198,\n",
       " 'with': 219,\n",
       " 'money': 134,\n",
       " 'for': 77,\n",
       " 'luggage': 124,\n",
       " 'too': 196,\n",
       " 'cancel': 45,\n",
       " 'booking': 39,\n",
       " 'tickets': 191,\n",
       " 'ticket': 190,\n",
       " 'cancellation': 46,\n",
       " '24th': 4,\n",
       " 'june': 112,\n",
       " 'do': 64,\n",
       " '12th': 1,\n",
       " 'dec': 61,\n",
       " 'booked': 38,\n",
       " 'those': 189,\n",
       " '03': 0,\n",
       " 'mar': 128,\n",
       " '2017': 2,\n",
       " 'need': 139,\n",
       " 'them': 186,\n",
       " 'be': 33,\n",
       " 'cancelled': 47,\n",
       " 'last': 116,\n",
       " 'minute': 130,\n",
       " 'cancelling': 48,\n",
       " 'reservation': 168,\n",
       " 'abel': 9,\n",
       " 'login': 121,\n",
       " 'forgot': 78,\n",
       " 'password': 157,\n",
       " 'error': 69,\n",
       " 'while': 217,\n",
       " 'logging': 120,\n",
       " 'failed': 71,\n",
       " 'trying': 200,\n",
       " 'help': 96,\n",
       " 'log': 118,\n",
       " 'but': 41,\n",
       " 'access': 11,\n",
       " 'account': 12,\n",
       " 'username': 202,\n",
       " 'remembering': 167,\n",
       " 'hi': 99,\n",
       " 'hello': 95,\n",
       " 'you': 222,\n",
       " 'there': 187,\n",
       " 'an': 22,\n",
       " 'issue': 108,\n",
       " 'hey': 98,\n",
       " 'good': 87,\n",
       " 'morning': 136,\n",
       " 'afternoon': 13,\n",
       " 'evening': 70,\n",
       " 'night': 144,\n",
       " 'goodmorning': 90,\n",
       " 'goodafternoon': 88,\n",
       " 'goodevening': 89,\n",
       " 'goodnight': 91,\n",
       " 'gm': 86,\n",
       " 'sir': 178,\n",
       " 'madam': 125,\n",
       " 'mam': 126,\n",
       " 'thanks': 183,\n",
       " 'thank': 182,\n",
       " 'it': 110,\n",
       " 'was': 207,\n",
       " 'helpful': 97,\n",
       " 'very': 205,\n",
       " 'nice': 143,\n",
       " 'ok': 150,\n",
       " 'see': 173,\n",
       " 'cool': 57,\n",
       " 'that': 184,\n",
       " 'really': 166,\n",
       " 'book': 37,\n",
       " 'payment': 159,\n",
       " 'alerts': 18,\n",
       " 'special': 179,\n",
       " 'assistance': 25,\n",
       " 'infants': 104,\n",
       " 'dogs': 65,\n",
       " 'general': 83,\n",
       " 'akslakalkd': 17,\n",
       " 'lak': 115,\n",
       " 'mmmm': 131,\n",
       " 'ppppp': 163,\n",
       " 'ooo': 155,\n",
       " 'abcddef': 8,\n",
       " 'billing': 35,\n",
       " 'which': 216,\n",
       " 'aircraft': 15,\n",
       " 'mom': 132,\n",
       " 'dad': 59,\n",
       " 'celebrations': 51,\n",
       " 'text': 181,\n",
       " 'birthdays': 36,\n",
       " 'insurance': 105,\n",
       " 'needed': 140,\n",
       " 'certificate': 52,\n",
       " 'emlsbxgolj': 67,\n",
       " 'xeydzyqqxh': 221,\n",
       " 'shuopizebx': 175,\n",
       " 'uvcvljqlsn': 203,\n",
       " 'gpjyuyekgu': 92,\n",
       " 'dbgirwlkgg': 60,\n",
       " 'rcocwhmcsz': 165,\n",
       " 'kkpakjyofl': 114,\n",
       " 'wuuaamgtky': 220,\n",
       " 'jkiupsgobh': 111,\n",
       " 'air': 14,\n",
       " 'canada': 44,\n",
       " 'rouge': 170,\n",
       " 'front': 81,\n",
       " 'food': 76,\n",
       " 'time': 193,\n",
       " 'and': 23,\n",
       " 'had': 93,\n",
       " 'one': 152,\n",
       " 'no': 145,\n",
       " 'at': 26,\n",
       " 'plane': 160,\n",
       " 'lot': 122,\n",
       " 'seat': 171,\n",
       " 'we': 209,\n",
       " 'all': 19,\n",
       " 'cabin': 42,\n",
       " 'crew': 58,\n",
       " 'way': 208,\n",
       " 'hour': 100,\n",
       " 'business': 40,\n",
       " 'class': 56,\n",
       " 'entertainment': 68,\n",
       " 'value': 204,\n",
       " 'from': 80,\n",
       " 'toronto': 197,\n",
       " 'seats': 172,\n",
       " 'were': 213,\n",
       " 'will': 218,\n",
       " 'never': 141,\n",
       " 'return': 169,\n",
       " 'delayed': 62,\n",
       " 'didn': 63,\n",
       " 'back': 28,\n",
       " 'gate': 82,\n",
       " 'don': 66,\n",
       " 'flown': 73,\n",
       " 'fly': 74,\n",
       " 'attendants': 27,\n",
       " 'sure': 180,\n",
       " 'passowrd': 156,\n",
       " 'pwd': 164,\n",
       " 'trouble': 199,\n",
       " 'loggin': 119,\n",
       " 'issues': 109,\n",
       " 'sign': 176,\n",
       " 'signing': 177,\n",
       " 'bagggage': 31,\n",
       " 'baggange': 30}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, (398, 223))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.vocabulary_),tf1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization\n",
    "Data normalization typically includes techniques like - generalizing dates, times, amount using regex. For instance we could group all date formats into a word as “Dates” similarly all amount paid or recieved could be replaced with the word “Money”. We could also “relabel” similar looking words. For ex - In our case - “Baggage” and “Luggage” means the same thing and hence they could be replaced into a single word. Normalization helps in 2 things - one making the data dense and the other is to prepare the data for any “unseen” variations. For eg - take the sentence. “My flight is on sunday and I want to check in”. If we build a model considering the previous sentence then when a new sentence is presented which says,“My flight is on Friday and I want to check in” the model may not perform well. Hence we replace any such words (in this case days of week) to possibly a common name (in this case “Dayofweek”) to generalize the model and make it more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few examples below on how regular expressions can help in preprocessing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"I want to be there on 19th\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to be there on datepp'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[0-9]+th\",\"datepp\",str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "str1 = \"I want to be there on 23-05-18\"\n",
    "str2 = \"I want to be there on 23-05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to be there on datepp\n",
      "I want to be there on datepp\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print (re.sub(\"[0-9]+[\\/-]+[0-9]+[\\/-]*[0-9]*\",\"datepp\",str1))\n",
    "print (re.sub(\"[0-9]+[\\/-]+[0-9]+[\\/-]*[0-9]*\",\"datepp\",str2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us apply the regular expression preprocessing to our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"line1\"]= df[\"line\"].str.replace('[0-9]+th','datepp')\n",
    "df[\"line1\"]= df[\"line1\"].str.replace('[0-9]+[\\/-]+[0-9]+[\\/-]*[0-9]*','datepp')\n",
    "df[\"line1\"] = df[\"line1\"].str.replace('[0-9]+','digitpp')\n",
    "df[\"line1\"]= df[\"line1\"].str.replace('[^A-Za-z]+',' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see an example of word replacements with a common group name. Here in replace we replace common similar meaning words to a common name. I created a file that maps similar words to a group. We shall now import and have a look at the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get preprocessing files to normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.read_csv('preprocess.csv',low_memory=False,encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>luggage</td>\n",
       "      <td>baggage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bags</td>\n",
       "      <td>baggage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>checkin</td>\n",
       "      <td>checkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chckin</td>\n",
       "      <td>checkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check in</td>\n",
       "      <td>checkin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word    class\n",
       "0   luggage  baggage\n",
       "1      bags  baggage\n",
       "2   checkin  checkin\n",
       "3    chckin  checkin\n",
       "4  check in  checkin"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to replace the words to the corresponding class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(l1):\n",
    "    l2=l1\n",
    "    for word in pp[\"word\"]:\n",
    "        if (l1.find(word)>=0):\n",
    "            newcl = list(pp.loc[pp.word.str.contains(word),\"class\"])[0]\n",
    "            l2 = l1.replace(word,newcl)\n",
    "            break;\n",
    "\n",
    "    return l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"line1\"] = df[\"line1\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = df.loc[:,\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to know the performance of the dataset on train and test we need to slice the dataset into train and test. Since we are dealing with text classes - we would have to do multi-class classification. This  means we will have to stratrified split the dataset into multiple classes so that each class gets enough representation in train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Startified sample to proceed before we do vectorizations - to take care of vocab not \"leaking\" to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(test_size=0.1,random_state=42,n_splits=1)\n",
    "\n",
    "for train_index, test_index in sss.split(df, tgt):\n",
    "    x_train, x_test = df[df.index.isin(train_index)], df[df.index.isin(test_index)]\n",
    "    y_train, y_test = df.loc[df.index.isin(train_index),\"class\"], df.loc[df.index.isin(test_index),\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358, 3), (358,), (40, 3), (40, 3))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape,x_test.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the TF-IDF Vectorizer on the train and test split we have. Remember to do this stop after train and test split. If this step is done before splitting chances are you are overestimating your accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.0001,analyzer=u'word',ngram_range=(1, 3),stop_words='english')\n",
    "tfidf_matrix_tr = tfidf_vectorizer.fit_transform(x_train[\"line1\"])\n",
    "\n",
    "tfidf_matrix_te = tfidf_vectorizer.transform(x_test[\"line1\"])\n",
    "\n",
    "x_train2= tfidf_matrix_tr.todense()\n",
    "x_test2 = tfidf_matrix_te.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358, 269), (358,), (40, 269), (40,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train2.shape,y_train.shape,x_test2.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply machine learning algorithm we will do a feature selection step and reduce the features to 40% of the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "selector = SelectPercentile(f_classif, percentile=40)\n",
    "selector.fit(x_train2, y_train)\n",
    "x_train3 = selector.fit_transform(x_train2, y_train)\n",
    "x_test3 = selector.transform(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358, 107), (40, 107))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train3.shape,x_test3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run our machine learning algorithm on the train dataset and test it on x_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(x_train3,y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measuring Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multiclass problem - class imbalance is a common occurrence where the distribution of categories could be biased towards a few of the top categories. Hence over and above the overall accuracy we want to test individual accuracies for different categories. This can be done using confusion matrix, precision, recall and F1 measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "pred=clf_log.predict(x_test3)\n",
    "\n",
    "print (accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7529993815708103"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, pred, average='macro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  5,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  4,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 10,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  8,  0],\n",
       "       [ 0,  0,  0,  0,  0,  2,  0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using XGBoost -Not in the chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build model using XGBoost. First step is to convert the dependent variable to label encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train1 = le.fit_transform(y_train)\n",
    "y_test1 = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = len(pd.Series(y_train).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing class weights so that the model penalizes errors for all classes equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "class_weights = list(class_weight.compute_class_weight('balanced',\n",
    "                                             np.unique(y_train1),\n",
    "                                             y_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7521008403361344,\n",
       " 3.6530612244897958,\n",
       " 0.9298701298701298,\n",
       " 1.2473867595818815,\n",
       " 0.5383458646616541,\n",
       " 0.7203219315895373,\n",
       " 3.6530612244897958]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weights = [0.08,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000,max_depth= 10,subsample= 0.5,colsample_bytree= 0.3\n",
    "                   ,class_weights = class_weights)\n",
    "xgb.fit(x_train3,y_train1)\n",
    "pred = xgb.predict(x_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Accuracies for XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print (accuracy_score(y_test1, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = le.inverse_transform(y_test1)\n",
    "pred2 = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  5,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  4,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 10,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  8,  0],\n",
       "       [ 0,  0,  0,  0,  0,  2,  0]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix as cf\n",
    "cf(y_test2, pred2, labels=None, sample_weight=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2-py36]",
   "language": "python",
   "name": "conda-env-Anaconda2-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
